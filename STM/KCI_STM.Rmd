---
title: "KCI 인문학 논문 STM 분석"
output: html_notebook
---

```{r include=FALSE}
library(stm)
library(stminsights)
library(tidyverse)
library(data.table)
library(lubridate)
library(zoo)
library(parallel)
```

```{r}
library(reticulate)
py_config()
```

## Load

파이썬 pickle 파일 로드

```{r include=FALSE}
df <- data.table(py_load_object('211016_master_preprocessed_token.pkl'))
```

```{r}
df %>% head()
```

```{r}
# 필요없는 컬럼 삭제
df <- df[,.(rank_citation,year,token_processed)]

# data.frame 타입으로 변환
df <- data.frame(df)

# 토큰 리스트 처리
df$token_processed <- map_chr(df$token_processed,str_c,collapse='  ')

# year
df$year <- as.integer(df$year)
  
# 날짜순 정렬(오름차순)
df <- df %>% arrange(year)

# 2004년부터 활용
df <- df %>% filter(year>=2004)

# 피인용수 로그화
# df$num_citation <- df$num_citation %>% log1p()
```

```{r}
# 데이터 확인
summary(df)
```

## STM

STM용으로 텍스트 전처리

```{r}
# 추가 불용어 리스트
custom_stopwords <- c('0', '고', '취', '다', '언', '입', '견', '용', '기', '대', '자', '상','이', '연','록','씨', '22','화','사', '명', '장', '일', '관', '지', '제','부', '주','제이','행','특','출','신','본','학','설','편','집','서','경','권','정','본고','세',
'년대')
```

```{r}
myprocess <- textProcessor(df$token_processed, metadata = df ,wordLengths=c(1,Inf),lowercase = F,
                           removenumbers = F, removepunctuation = F, removestopwords = F, stem = F, customstopwords = custom_stopwords)
myprocess
length(myprocess$docs.removed)
```

STM용으로 자료형 변환

```{r}
# N개 이상의 문서에서 등장한 단어만 사용(lower.thresh)
out <- prepDocuments(myprocess$documents, myprocess$vocab, myprocess$meta,lower.thresh = 1000)
```

파싱 및 토크나이징 완료 변수 저장

```{r}
save.image("211111_df_morp.RData")
```

### Model 1 = \~s(year)

최적 토픽 개수 탐색(멀티코어는 리눅스만 가능)

```{r}
model1_searchK <- searchK(out$documents, out$vocab, K = c(8:30),
                                prevalence = ~s(year),
                                data = out$meta, init.type="Spectral"
                                  ,cores=detectCores()-1)
saveRDS(model1_searchK,'model1_searchK.rds')
```

```{r}
plot(model1_searchK)
```

```{r}
model1_searchK
```

모델링

```{r}
stm_model1 <- stm(out$documents, out$vocab, K=15,
              prevalence= ~s(year),
              data=out$meta, init.type="Spectral",seed=2021,
              verbose = F)
```

```{r}
summary(stm_model1)
```

```{r}
labelTopics(stm_model1, n=10)
```

```{r}
plot(stm_model1,type='summary',labeltype = 'frex',n=10)
```

모델 효과 추정

```{r}
m1_K <- stm_model1$settings$dim$K
stm_effect_model1 <-  estimateEffect(1:m1_K ~s(year),
                                 stm_model1, meta = out$meta, uncertainty = "Global")
```

```{r}
summary(stm_effect_model1, topics=c(1:m1_K))
```

```{r}
# 시계열 시각화(모든 토픽)
plot.estimateEffect(stm_effect_model1,model=stm, covariate = "year", 
                    topics = c(1:m1_K), method = "continuous")
```

```{r}
#### 시간에 따른 토픽 비율 변화 (토픽별로)
stm_label<- labelTopics(stm_model1, n = 18)
# stm_custom_label <- c('접종순서','거리두기 단계','국내 감염 상황','생활/문화/교육','관련연구/기술',
#                                       '지원정책','관련주','백신 승인','미국 대선','경제 전망','정부/청와대',
#                                       '해외 감염 상황','접종후속대책','변이 바이러스','국제협력','증상/전파','백신/치료제 개발','부작용')

par(mfrow=c(3,2))
j <- 1
for (i in c(1:m1_K))
{
  plot(stm_effect_model1, "year", method = "continuous", topics = i, printlegend = F,
  # main = stm_custom_label[j], xaxt = "n")
  #main = paste(paste0('T', i,':'),paste(stm_custom_label[i], collapse = ", "),sep=' '),
  #xaxt ="n")
  
  # 토픽 이름대신 keyword로 표현하고 싶으면 아래 main 활용 
  main =  paste('topic', i,paste(stm_label$frex[i,1:4], collapse = ", "),sep=' '))
  
  yearseq <- seq(from=as.Date('2000-01-01'), to=as.Date('2019-12-31'),by='year')
yearnames <- year(yearseq)
axis(1,at=as.numeric(yearseq) - min(as.numeric(yearseq)),labels=yearnames)
  
  j <- j+1

}
```

```{r}
# 토픽 네트워크
# plot(topicCorr(stm_model1),vlabels =stm_custom_label, vertex.label.cex = 0.55)
plot(topicCorr(stm_model1), vertex.label.cex = 0.55)
```

#### Stminsights(Model1)

```{r}
run_stminsights()
```

### Model 2 = \~s(rank_citation)

최적 개수 탐색

```{r}
model2_searchK <- searchK(out$documents, out$vocab, K = c(4:20),
                                prevalence = ~s(num_citation),
                                data = out$meta, init.type="Spectral"
                                  ,cores=detectCores()-1)
saveRDS(model2_searchK,'model2_searchK.rds')
```

```{r}
plot(model2_searchK)
```

```{r}
model2_searchK
```

모델링

```{r}
stm_model2 <- stm(out$documents, out$vocab, K=10,
              prevalence= ~s(rank_citation),
              data=out$meta, init.type="Spectral",seed=2021,
              verbose = F)
```

모델 효과 추정

```{r}
m2_K <- stm_model2$settings$dim$K
stm_effect_model2 <-  estimateEffect(1:m2_K ~s(rank_citation),
                                 stm_model2, meta = out$meta, uncertainty = "Global")
```

```{r}
summary(stm_effect_model2, topics=c(1:m2_K))
```

```{r}
# 시계열 시각화(모든 토픽)
plot.estimateEffect(stm_effect_model2,model=stm, covariate = "rank_citation", 
                    topics = c(1:m2_K), method = "continuous")
```

```{r}
#### 시간에 따른 토픽 비율 변화 (토픽별로)
stm_label<- labelTopics(stm_model2, n = 18)
# stm_custom_label <- c('접종순서','거리두기 단계','국내 감염 상황','생활/문화/교육','관련연구/기술',
#                                       '지원정책','관련주','백신 승인','미국 대선','경제 전망','정부/청와대',
#                                       '해외 감염 상황','접종후속대책','변이 바이러스','국제협력','증상/전파','백신/치료제 개발','부작용')

par(mfrow=c(3,2))
j <- 1
for (i in c(1:m2_K))
{
  plot(stm_effect_model2, "rank_citation", method = "continuous", topics = i, printlegend = F,
  # main = stm_custom_label[j], xaxt = "n")
  #main = paste(paste0('T', i,':'),paste(stm_custom_label[i], collapse = ", "),sep=' '),
  #xaxt ="n")
  
  # 토픽 이름대신 keyword로 표현하고 싶으면 아래 main 활용 
  main =  paste('topic', i,paste(stm_label$frex[i,1:4], collapse = ", "),sep=' '))
  
#   yearseq <- seq(from=as.Date('2000-01-01'), to=as.Date('2019-12-31'),by='year')
# yearnames <- year(yearseq)
# axis(1,at=as.numeric(yearseq) - min(as.numeric(yearseq)),labels=yearnames)
  
  j <- j+1

}
```
